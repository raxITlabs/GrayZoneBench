{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69432f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HarmBench datasets ...\n",
      "Prepared splits (original schemas):\n",
      " - standard: 200 ['prompt', 'category']\n",
      " - contextual: 100 ['prompt', 'context', 'category']\n",
      "CSV directory: /Users/adeshgairola/Documents/Test-temp-folder/openai-safety-bench/dataset/grayzone_csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login, create_repo\n",
    "\n",
    "# Load environment variables from a .env file (root at ../.env, fallback to local)\n",
    "def load_env_file(path: str) -> None:\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as env_file:\n",
    "            for raw_line in env_file:\n",
    "                line = raw_line.strip()\n",
    "                if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "                    continue\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip().strip('\"').strip(\"'\")\n",
    "                if key and key not in os.environ:\n",
    "                    os.environ[key] = value\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# First try root .env at project root, then a local .env next to the notebook\n",
    "load_env_file(\"../.env\")\n",
    "load_env_file(\".env\")\n",
    "\n",
    "# Authenticate with Hugging Face using an env var token (HF_TOKEN or HUGGINGFACE_HUB_TOKEN)\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise RuntimeError(\n",
    "        \"Missing Hugging Face token. Put HF_TOKEN in the repo root .env or set it in your environment to push to the Hub.\"\n",
    "    )\n",
    "login(token=hf_token)\n",
    "\n",
    "# Ensure the target dataset repo exists\n",
    "repo_id = \"raxITLabs/GrayZone\"\n",
    "create_repo(repo_id, repo_type=\"dataset\", exist_ok=True, token=hf_token)\n",
    "\n",
    "# Step 1 — Load HarmBench datasets (no push here)\n",
    "print(\"Loading HarmBench datasets ...\")\n",
    "ds_standard = load_dataset(\"walledai/HarmBench\", \"standard\")\n",
    "ds_contextual = load_dataset(\"walledai/HarmBench\", \"contextual\")\n",
    "\n",
    "# Keep original schemas (no added columns)\n",
    "standard_train = ds_standard[\"train\"]\n",
    "contextual_train = ds_contextual[\"train\"]\n",
    "\n",
    "# Keep splits for CSV export/import steps\n",
    "print(\"Prepared splits (original schemas):\")\n",
    "print(\" - standard:\", len(standard_train), standard_train.column_names)\n",
    "print(\" - contextual:\", len(contextual_train), contextual_train.column_names)\n",
    "\n",
    "# Where CSVs will be written/read (next cells use this path)\n",
    "csv_dir = \"./grayzone_csv\"\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "print(\"CSV directory:\", os.path.abspath(csv_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c378091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 200/200 [00:00<00:00, 13863.86 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 56817.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered counts:\n",
      " - standard: 40\n",
      " - contextual: 27\n",
      "Wrote:\n",
      " - /Users/adeshgairola/Documents/Test-temp-folder/openai-safety-bench/dataset/grayzone_csv/standard.cybercrime_intrusion.csv\n",
      " - /Users/adeshgairola/Documents/Test-temp-folder/openai-safety-bench/dataset/grayzone_csv/contextual.cybercrime_intrusion.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Filter category == 'cybercrime_intrusion' and export to CSV\n",
    "# Writes filtered CSVs under csv_dir\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Filter both splits by category\n",
    "standard_filtered = standard_train.filter(lambda ex: ex[\"category\"] == \"cybercrime_intrusion\")\n",
    "contextual_filtered = contextual_train.filter(lambda ex: ex[\"category\"] == \"cybercrime_intrusion\")\n",
    "\n",
    "# Output paths\n",
    "standard_csv_path = os.path.join(csv_dir, \"standard.cybercrime_intrusion.csv\")\n",
    "contextual_csv_path = os.path.join(csv_dir, \"contextual.cybercrime_intrusion.csv\")\n",
    "\n",
    "# Export to CSV\n",
    "standard_filtered.to_pandas().to_csv(standard_csv_path, index=False)\n",
    "contextual_filtered.to_pandas().to_csv(contextual_csv_path, index=False)\n",
    "\n",
    "print(\"Filtered counts:\")\n",
    "print(\" - standard:\", len(standard_filtered))\n",
    "print(\" - contextual:\", len(contextual_filtered))\n",
    "print(\"Wrote:\")\n",
    "print(\" -\", os.path.abspath(standard_csv_path))\n",
    "print(\" -\", os.path.abspath(contextual_csv_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe2bc6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard columns: ['prompt', 'category']\n",
      "contextual columns: ['prompt', 'context', 'category']\n",
      "Ready to push two configs: standard (train) and contextual (train)\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — Re-import edited CSVs preserving each split's original schema\n",
    "# Edit the CSVs in `csv_dir` manually, then run this cell to re-import.\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Read edited CSVs\n",
    "standard_csv_path = os.path.join(csv_dir, \"standard.cybercrime_intrusion.csv\")\n",
    "contextual_csv_path = os.path.join(csv_dir, \"contextual.cybercrime_intrusion.csv\")\n",
    "\n",
    "standard_df = pd.read_csv(standard_csv_path)\n",
    "contextual_df = pd.read_csv(contextual_csv_path)\n",
    "\n",
    "# Do not add or reorder columns — keep as-is for each split\n",
    "standard_ds = Dataset.from_pandas(standard_df, preserve_index=False)\n",
    "contextual_ds = Dataset.from_pandas(contextual_df, preserve_index=False)\n",
    "\n",
    "print(\"standard columns:\", standard_ds.column_names)\n",
    "print(\"contextual columns:\", contextual_ds.column_names)\n",
    "\n",
    "# Build two separate DatasetDicts (one per config) since schemas differ\n",
    "combined_standard = DatasetDict({\"train\": standard_ds})\n",
    "combined_contextual = DatasetDict({\"train\": contextual_ds})\n",
    "\n",
    "print(\"Ready to push two configs: standard (train) and contextual (train)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade290c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing both configs directly to raxITLabs/GrayZone main ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 327.02ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.55kB / 3.55kB, 2.96kB/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.55kB / 3.55kB, 2.22kB/s  \n",
      "New Data Upload                         : 100%|██████████| 3.55kB / 3.55kB, 2.22kB/s  \n",
      "                                        : 100%|██████████| 3.55kB / 3.55kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.50s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1133.90ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 24.1kB / 24.1kB, 20.0kB/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 24.1kB / 24.1kB, 15.0kB/s  \n",
      "New Data Upload                         : 100%|██████████| 24.1kB / 24.1kB, 15.0kB/s  \n",
      "                                        : 100%|██████████| 24.1kB / 24.1kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.48s/ shards]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed directly to main.\n",
      "Done. Check: https://huggingface.co/datasets/raxITLabs/GrayZone\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Simple push: direct to org main; on 403, fall back to your user repo (no PR/branches)\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Detect current user for fallback\n",
    "who = api.whoami(token=hf_token)\n",
    "user_name = who.get(\"name\") or who.get(\"email\") or \"user\"\n",
    "\n",
    "try:\n",
    "    print(f\"Pushing both configs directly to {repo_id} main ...\")\n",
    "    combined_standard.push_to_hub(\n",
    "        repo_id,\n",
    "        config_name=\"standard\",\n",
    "        token=hf_token,\n",
    "        private=False,\n",
    "        revision=\"main\",\n",
    "        create_pr=False,\n",
    "        commit_message=\"Upload GrayZone standard (train) to main\",\n",
    "    )\n",
    "    combined_contextual.push_to_hub(\n",
    "        repo_id,\n",
    "        config_name=\"contextual\",\n",
    "        token=hf_token,\n",
    "        private=False,\n",
    "        revision=\"main\",\n",
    "        create_pr=False,\n",
    "        commit_message=\"Upload GrayZone contextual (train) to main\",\n",
    "    )\n",
    "    pushed_repo = repo_id\n",
    "    print(\"Pushed directly to main.\")\n",
    "except HfHubHTTPError as e:\n",
    "    print(f\"Direct push to org main failed with {e}. Falling back to user namespace.\")\n",
    "    fallback_repo_id = f\"{user_name}/GrayZone\"\n",
    "    create_repo(fallback_repo_id, repo_type=\"dataset\", exist_ok=True, token=hf_token)\n",
    "\n",
    "    combined_standard.push_to_hub(\n",
    "        fallback_repo_id,\n",
    "        config_name=\"standard\",\n",
    "        token=hf_token,\n",
    "        private=False,\n",
    "        revision=\"main\",\n",
    "        create_pr=False,\n",
    "        commit_message=\"Upload GrayZone standard (train) to user main\",\n",
    "    )\n",
    "    combined_contextual.push_to_hub(\n",
    "        fallback_repo_id,\n",
    "        config_name=\"contextual\",\n",
    "        token=hf_token,\n",
    "        private=False,\n",
    "        revision=\"main\",\n",
    "        create_pr=False,\n",
    "        commit_message=\"Upload GrayZone contextual (train) to user main\",\n",
    "    )\n",
    "    pushed_repo = fallback_repo_id\n",
    "    print(\"Pushed to user repo main.\")\n",
    "\n",
    "print(\"Done. Check:\", f\"https://huggingface.co/datasets/{pushed_repo}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
