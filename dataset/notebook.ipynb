{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69432f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HarmBench datasets ...\n",
      "Prepared splits (original schemas):\n",
      " - standard: 200 ['prompt', 'category']\n",
      " - contextual: 100 ['prompt', 'context', 'category']\n",
      "CSV directory: /Users/adeshgairola/Documents/Test-temp-folder/openai-safety-bench/dataset/grayzone_csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login, create_repo\n",
    "\n",
    "# Load environment variables from a .env file (root at ../.env, fallback to local)\n",
    "def load_env_file(path: str) -> None:\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as env_file:\n",
    "            for raw_line in env_file:\n",
    "                line = raw_line.strip()\n",
    "                if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "                    continue\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip().strip('\"').strip(\"'\")\n",
    "                if key and key not in os.environ:\n",
    "                    os.environ[key] = value\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# First try root .env at project root, then a local .env next to the notebook\n",
    "load_env_file(\"../.env\")\n",
    "load_env_file(\".env\")\n",
    "\n",
    "# Authenticate with Hugging Face using an env var token (HF_TOKEN or HUGGINGFACE_HUB_TOKEN)\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise RuntimeError(\n",
    "        \"Missing Hugging Face token. Put HF_TOKEN in the repo root .env or set it in your environment to push to the Hub.\"\n",
    "    )\n",
    "login(token=hf_token)\n",
    "\n",
    "# Ensure the target dataset repo exists\n",
    "repo_id = \"raxITLabs/GrayZone\"\n",
    "create_repo(repo_id, repo_type=\"dataset\", exist_ok=True, token=hf_token)\n",
    "\n",
    "# Step 1 — Load HarmBench datasets (no push here)\n",
    "print(\"Loading HarmBench datasets ...\")\n",
    "ds_standard = load_dataset(\"walledai/HarmBench\", \"standard\")\n",
    "ds_contextual = load_dataset(\"walledai/HarmBench\", \"contextual\")\n",
    "\n",
    "# Keep original schemas (no added columns)\n",
    "standard_train = ds_standard[\"train\"]\n",
    "contextual_train = ds_contextual[\"train\"]\n",
    "\n",
    "# Keep splits for CSV export/import steps\n",
    "print(\"Prepared splits (original schemas):\")\n",
    "print(\" - standard:\", len(standard_train), standard_train.column_names)\n",
    "print(\" - contextual:\", len(contextual_train), contextual_train.column_names)\n",
    "\n",
    "# Where CSVs will be written/read (next cells use this path)\n",
    "csv_dir = \"./grayzone_csv\"\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "print(\"CSV directory:\", os.path.abspath(csv_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c378091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      " - /Users/adeshgairola/Documents/Test-temp-folder/openai-safety-bench/dataset/grayzone_csv/standard.csv\n",
      " - /Users/adeshgairola/Documents/Test-temp-folder/openai-safety-bench/dataset/grayzone_csv/contextual.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Export splits to CSV for manual editing\n",
    "# This writes two CSV files: standard.csv and contextual.csv under csv_dir\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "standard_csv_path = os.path.join(csv_dir, \"standard.csv\")\n",
    "contextual_csv_path = os.path.join(csv_dir, \"contextual.csv\")\n",
    "\n",
    "# Convert to pandas and export\n",
    "pd.DataFrame(standard_train).to_csv(standard_csv_path, index=False)\n",
    "pd.DataFrame(contextual_train).to_csv(contextual_csv_path, index=False)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" -\", os.path.abspath(standard_csv_path))\n",
    "print(\" -\", os.path.abspath(contextual_csv_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe2bc6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard columns: ['prompt', 'category']\n",
      "contextual columns: ['prompt', 'context', 'category']\n",
      "Ready to push two configs: standard (train) and contextual (train)\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — Re-import edited CSVs preserving each split's original schema\n",
    "# Edit the CSVs in `csv_dir` manually, then run this cell to re-import.\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Read edited CSVs\n",
    "standard_csv_path = os.path.join(csv_dir, \"standard.csv\")\n",
    "contextual_csv_path = os.path.join(csv_dir, \"contextual.csv\")\n",
    "\n",
    "standard_df = pd.read_csv(standard_csv_path)\n",
    "contextual_df = pd.read_csv(contextual_csv_path)\n",
    "\n",
    "# Do not add or reorder columns — keep as-is for each split\n",
    "standard_ds = Dataset.from_pandas(standard_df, preserve_index=False)\n",
    "contextual_ds = Dataset.from_pandas(contextual_df, preserve_index=False)\n",
    "\n",
    "print(\"standard columns:\", standard_ds.column_names)\n",
    "print(\"contextual columns:\", contextual_ds.column_names)\n",
    "\n",
    "# Build two separate DatasetDicts (one per config) since schemas differ\n",
    "combined_standard = DatasetDict({\"train\": standard_ds})\n",
    "combined_contextual = DatasetDict({\"train\": contextual_ds})\n",
    "\n",
    "print(\"Ready to push two configs: standard (train) and contextual (train)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ade290c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing 'standard' config ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1845.27ba/s]\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-689aa399-298ae00070c4f0515a59fd11;fb42d120-acf2-4c7d-9b2e-ba741784f42f)\n\n403 Forbidden: Authorization error..\nCannot access content at: https://huggingface.co/api/datasets/raxITLabs/GrayZone/preupload/main?create_pr=1.\nMake sure your token has the correct permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/api/datasets/raxITLabs/GrayZone/preupload/main?create_pr=1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 4 — Push edited data to Hugging Face Hub as two configs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We push `standard` and `contextual` separately to preserve original schemas\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPushing \u001b[39m\u001b[33m'\u001b[39m\u001b[33mstandard\u001b[39m\u001b[33m'\u001b[39m\u001b[33m config ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mcombined_standard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstandard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUpload edited GrayZone standard (train) via PR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPushing \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcontextual\u001b[39m\u001b[33m'\u001b[39m\u001b[33m config ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m combined_contextual.push_to_hub(\n\u001b[32m     16\u001b[39m     repo_id,\n\u001b[32m     17\u001b[39m     config_name=\u001b[33m\"\u001b[39m\u001b[33mcontextual\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     commit_message=\u001b[33m\"\u001b[39m\u001b[33mUpload edited GrayZone contextual (train) via PR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/datasets/dataset_dict.py:1758\u001b[39m, in \u001b[36mDatasetDict.push_to_hub\u001b[39m\u001b[34m(self, repo_id, config_name, set_default, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files, num_proc)\u001b[39m\n\u001b[32m   1756\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPushing split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to the Hub.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1758\u001b[39m split_additions, uploaded_size, dataset_nbytes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_push_parquet_shards_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1770\u001b[39m additions += split_additions\n\u001b[32m   1771\u001b[39m total_uploaded_size += uploaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:5613\u001b[39m, in \u001b[36mDataset._push_parquet_shards_to_hub\u001b[39m\u001b[34m(self, repo_id, data_dir, split, token, revision, create_pr, max_shard_size, num_shards, embed_external_files, num_proc)\u001b[39m\n\u001b[32m   5603\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.nullcontext() \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc <= \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m Pool(num_proc) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[32m   5604\u001b[39m     update_stream = (\n\u001b[32m   5605\u001b[39m         Dataset._push_parquet_shards_to_hub_single(**kwargs_iterable[\u001b[32m0\u001b[39m])\n\u001b[32m   5606\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5611\u001b[39m         )\n\u001b[32m   5612\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5613\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5615\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:5529\u001b[39m, in \u001b[36mDataset._push_parquet_shards_to_hub_single\u001b[39m\u001b[34m(self, job_id, num_jobs, repo_id, data_dir, split, token, revision, create_pr, num_shards, embed_external_files)\u001b[39m\n\u001b[32m   5527\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buffer\n\u001b[32m   5528\u001b[39m shard_addition = CommitOperationAdd(path_in_repo=shard_path_in_repo, path_or_fileobj=parquet_content)\n\u001b[32m-> \u001b[39m\u001b[32m5529\u001b[39m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5531\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshard_addition\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5536\u001b[39m additions.append(shard_addition)\n\u001b[32m   5537\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:4459\u001b[39m, in \u001b[36mHfApi.preupload_lfs_files\u001b[39m\u001b[34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[39m\n\u001b[32m   4457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(additions_no_upload_mode) > \u001b[32m0\u001b[39m:\n\u001b[32m   4458\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4459\u001b[39m         \u001b[43m_fetch_upload_modes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4460\u001b[39m \u001b[43m            \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditions_no_upload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4461\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4466\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4467\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4468\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4469\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4470\u001b[39m         e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/huggingface_hub/_commit_api.py:692\u001b[39m, in \u001b[36m_fetch_upload_modes\u001b[39m\u001b[34m(additions, repo_type, repo_id, headers, revision, endpoint, create_pr, gitignore_content)\u001b[39m\n\u001b[32m    684\u001b[39m     payload[\u001b[33m\"\u001b[39m\u001b[33mgitIgnore\u001b[39m\u001b[33m\"\u001b[39m] = gitignore_content\n\u001b[32m    686\u001b[39m resp = get_session().post(\n\u001b[32m    687\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/preupload/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    688\u001b[39m     json=payload,\n\u001b[32m    689\u001b[39m     headers=headers,\n\u001b[32m    690\u001b[39m     params={\u001b[33m\"\u001b[39m\u001b[33mcreate_pr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    691\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m preupload_info = _validate_preupload_info(resp.json())\n\u001b[32m    694\u001b[39m upload_modes.update(**{file[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m]: file[\u001b[33m\"\u001b[39m\u001b[33muploadMode\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m preupload_info[\u001b[33m\"\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m\"\u001b[39m]})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Test-temp-folder/openai-safety-bench/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:473\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    468\u001b[39m     message = (\n\u001b[32m    469\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n\u001b[32m    476\u001b[39m     range_header = response.request.headers.get(\u001b[33m\"\u001b[39m\u001b[33mRange\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: (Request ID: Root=1-689aa399-298ae00070c4f0515a59fd11;fb42d120-acf2-4c7d-9b2e-ba741784f42f)\n\n403 Forbidden: Authorization error..\nCannot access content at: https://huggingface.co/api/datasets/raxITLabs/GrayZone/preupload/main?create_pr=1.\nMake sure your token has the correct permissions."
     ]
    }
   ],
   "source": [
    "# Step 4 — Push edited data to Hugging Face Hub as two configs\n",
    "# We push `standard` and `contextual` separately to preserve original schemas\n",
    "\n",
    "print(\"Pushing 'standard' config ...\")\n",
    "combined_standard.push_to_hub(\n",
    "    repo_id,\n",
    "    config_name=\"standard\",\n",
    "    token=hf_token,\n",
    "    private=False,\n",
    "    create_pr=True,\n",
    "    commit_message=\"Upload edited GrayZone standard (train) via PR\",\n",
    ")\n",
    "\n",
    "print(\"Pushing 'contextual' config ...\")\n",
    "combined_contextual.push_to_hub(\n",
    "    repo_id,\n",
    "    config_name=\"contextual\",\n",
    "    token=hf_token,\n",
    "    private=False,\n",
    "    create_pr=True,\n",
    "    commit_message=\"Upload edited GrayZone contextual (train) via PR\",\n",
    ")\n",
    "\n",
    "print(\"Upload complete (PRs created if you lack write access):\", f\"https://huggingface.co/datasets/{repo_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e15e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
